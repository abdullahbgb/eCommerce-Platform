# -*- coding: utf-8 -*-
"""AIP Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ovKV8JFvALTh-_XbIjWS8khGW2RjJNPz
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")
import geohash2 as geohash
from geopy.geocoders import Nominatim
from math import radians, cos, sin, asin, sqrt
from geopy.distance import geodesic
from sklearn.svm import SVC
svm_algo = SVC(C=1.0, kernel='rbf', gamma=1e-3)
from sklearn.linear_model import LogisticRegression as LogR
from sklearn.ensemble import RandomForestClassifier as RF
from sklearn.ensemble import GradientBoostingClassifier as GBDT
from xgboost import XGBClassifier as XGB
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import precision_recall_fscore_support
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint
from sklearn.model_selection  import train_test_split
from sklearn.tree import DecisionTreeClassifier as tree
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

"""## Reading and printing all csv files individually
- if the files.upload above isn't working, this will make sure that the files are being uploaded to the project

"""

customers = pd.read_csv("olist_customers_dataset.csv")
order_items = pd.read_csv("olist_order_items_dataset.csv")
order_payments = pd.read_csv("olist_order_payments_dataset.csv")
order_reviews = pd.read_csv("olist_order_reviews_dataset.csv")
orders = pd.read_csv("olist_orders_dataset.csv")
products = pd.read_csv("olist_products_dataset.csv")
sellers = pd.read_csv("olist_sellers_dataset.csv")
geolocation = pd.read_csv("olist_geolocation_dataset.csv")
translation = pd.read_csv("product_category_name_translation.csv")

print("\n Customers Dataset \n\n", customers.isnull().sum())
print("\n Geolocation Dataset \n\n", geolocation.isnull().sum())
print("\n Order Items Dataset \n\n", order_items.isnull().sum())
print("\n Order Payments Dataset \n\n", order_payments.isnull().sum())
print("\n Order Reviews Dataset \n\n", order_reviews.isnull().sum())
print("\n Orders Dataset \n\n", orders.isnull().sum())
print("\n Products Dataset \n\n", products.isnull().sum())
print("\n Sellers Dataset \n\n", sellers.isnull().sum())

import matplotlib.pyplot as plt
import seaborn as sns

print("\n Customers Dataset \n\n", customers.info())
print("\n Order Items Dataset \n\n", order_items.info())
print("\n Order Payments Dataset \n\n", order_payments.info())
print("\n Order Reviews Dataset \n\n", order_reviews.info())
print("\n Orders Dataset \n\n", orders.info())
print("\n Products Dataset \n\n", products.info())
print("\n Sellers Dataset \n\n", sellers.info())

"""The above has completed the data import and overview

---

##Calculate and label the volumetric weight of the products

Calculating the volumetric weight of each product and assigning a rating to it.

$\text{Volumetric Weight} = \frac{\text{Length} \times \text{Width} \times \text{Height}}{5}$

Calculating this we get the volumetric weight in grams. After this we assign weights to each product's volumetric weight based upon this:-

$
\begin{array}{|c|c|}
\hline
Range & Weight \\
\hline
Below 500g &  1 \\
\hline
500g - 1500g &  2 \\
\hline
1500g - 3000g &  3 \\
\hline
3000g - 5000g &  4 \\
\hline
Above 5000g &  5 \\
\hline
\end{array}
$
"""

#Calculating the volumetric weight
products['product_volumetric_weight_g'] = (products['product_length_cm'] * products['product_width_cm'] * products['product_height_cm'])/5

#Ranges are Below 500g, 500-1500g, 1500-3000g, 3000-5000g and above 5000g
def volumetric_weight_rating(volumetric_weight):
    if volumetric_weight < 500:
      return 1
    elif volumetric_weight < 1500:
      return 2
    elif volumetric_weight < 3000:
      return 3
    elif volumetric_weight < 5000:
      return 4
    else:
      return 5

#Applying the volumetric weight rating
products['volumetric_weight_rating'] = products['product_volumetric_weight_g'].apply(volumetric_weight_rating)
print(products)

# Calculate product volume
products['product_volume_cm3'] = products['product_length_cm'] * products['product_width_cm'] * products['product_height_cm']

# Define a function to rate
print(products[['product_weight_g','product_volume_cm3','product_description_lenght']].describe())

def weight_rating(weight):
    if weight < 300:
        return 1
    elif weight < 700:
        return 2
    elif weight < 1900:
        return 3
    else:
        return 4

def volume_rating(volume):
    if volume < 2880:
        return 1
    elif volume < 6840:
        return 2
    elif volume < 18480:
        return 3
    else:
        return 4

def descleng_rating(des_lenghth):
    if des_lenghth < 339:
        return 1
    elif des_lenghth < 595:
        return 2
    elif des_lenghth < 972:
        return 3
    else:
        return 4

# Application rating function
products['weight_rating'] = products['product_weight_g'].apply(weight_rating)
products['volume_rating'] = products['product_volume_cm3'].apply(volume_rating)
products['descleng_rating'] = products['product_description_lenght'].apply(volume_rating)

# Inspection: Output processed data
volume_rating_counts = products['volume_rating'].value_counts().sort_index()
weight_rating_counts = products['weight_rating'].value_counts().sort_index()
descleng_rating_counts = products['descleng_rating'].value_counts().sort_index()
print(volume_rating_counts,weight_rating_counts,descleng_rating_counts)

"""## Translation
- Using dictionary to directly translate the product names to english without doing that on excel
"""

#Translate to english
# Create the translation dictionary
Translation_dict = dict(zip(translation['product_category_name'], translation['product_category_name_english']))

# Map the 'product_category_name' to its English equivalent
translation['product_category_english'] = translation['product_category_name'].map(Translation_dict)

# Handle missing translations by falling back to the original 'product_category_name'
translation['product_category_english'] = translation['product_category_name_english'].fillna(translation['product_category_name'])

# Check the output
print(translation[['product_category_name', 'product_category_english']].head())

"""##Calculate the distance between zipcodes

Calculating the distance between the zipcodes of the customer with the zipcode of the seller using the latitude and longitude data provided in the geolocation dataset.
"""

# Merge the orders and customers datasets on the customer ID to get customer zip code prefixes in the orders DataFrame.
orders = pd.merge(orders, customers[['customer_id', 'customer_zip_code_prefix']], on='customer_id', how='left')

# Merge the orders dataset with sellers on the 'order_id' column
# This will bring the 'seller_zip_code_prefix' into the orders dataset
orders = pd.merge(orders, order_items[['order_id', 'seller_id']], on='order_id', how='left')
orders = pd.merge(orders, sellers[['seller_id', 'seller_zip_code_prefix']], on='seller_id', how='left')

# Create a dictionary to efficiently map zip codes to coordinates
# Group by 'geolocation_zip_code_prefix' and aggregate coordinates (e.g., using mean)
geolocation_dict = geolocation.groupby('geolocation_zip_code_prefix')[['geolocation_lat', 'geolocation_lng']].mean().to_dict('index')

def calculate_distance(row):
    customer_zip = row['customer_zip_code_prefix']
    seller_zip = row['seller_zip_code_prefix']

# Handle missing zip codes:
    if pd.isna(customer_zip) or pd.isna(seller_zip):
        return np.nan  # Return NaN if either zip code is missing

    # Access the latitude and longitude values directly
    try:  # Try to access coordinates, if available
        customer_coords = (geolocation_dict[customer_zip]['geolocation_lat'], geolocation_dict[customer_zip]['geolocation_lng'])
        seller_coords = (geolocation_dict[seller_zip]['geolocation_lat'], geolocation_dict[seller_zip]['geolocation_lng'])
        return geodesic(customer_coords, seller_coords).km
    except KeyError:  # Handle KeyError if zip code not in dictionary
        return np.nan  # Return NaN if zip code not found

    return geodesic(customer_coords, seller_coords).km

# Calculate distances for each order
orders['distance_km'] = orders.apply(calculate_distance, axis=1)

"""##Calculate the duration of each process

Due to logical errors found in the timestamp of the original data during the initial inspection, such as customers receiving packages earlier than merchants, this resulted in over 3000 instances of such data. They are returned as 0
"""

orders['order_purchase_timestamp'] = pd.to_datetime(orders['order_purchase_timestamp'])
orders['order_approved_at'] = pd.to_datetime(orders['order_approved_at'])
orders['order_delivered_carrier_date'] = pd.to_datetime(orders['order_delivered_carrier_date'])
orders['order_delivered_customer_date'] = pd.to_datetime(orders['order_delivered_customer_date'])
orders['order_estimated_delivery_date'] = pd.to_datetime(orders['order_estimated_delivery_date'])

order_reviews['review_creation_date'] = pd.to_datetime(order_reviews['review_creation_date'])
order_reviews['review_answer_timestamp'] = pd.to_datetime(order_reviews['review_answer_timestamp'])

# Calculate the duration (in hours)
orders['approval_duration_hours'] = (orders['order_approved_at'] - orders['order_purchase_timestamp']).dt.total_seconds() / 3600

# Order shipping time from merchant
orders['delivery_fromseller'] = (
    np.where(
        orders['order_delivered_carrier_date'] >= orders['order_approved_at'],
        (orders['order_delivered_carrier_date'] - orders['order_approved_at']).dt.total_seconds() / 3600,
        0
    )
)

# Delivery time of express delivery
orders['delivery_tocusto'] = (
    np.where(
        orders['order_delivered_customer_date'] >= orders['order_delivered_carrier_date'],
        (orders['order_delivered_customer_date'] - orders['order_delivered_carrier_date']).dt.total_seconds() / 3600,
        0
    )
)

#Calculate the duration of the review answer
order_reviews['review_toanswer'] = (order_reviews['review_answer_timestamp'] - order_reviews['review_creation_date']).dt.total_seconds() / 3600

# Compare order_delivered_custom_date and order_ estimated d_delivery_date
orders['estimated_delivery_later'] = (orders['order_estimated_delivery_date'] > orders['order_delivered_customer_date']).astype(int) #Mark 1 if arrive in time

"""##Label textual variables"""

#Mark the content of the review
order_reviews['title_flag'] = order_reviews['review_comment_title'].notna().astype(int)
order_reviews['message_flag'] = order_reviews['review_comment_message'].notna().astype(int)

#Mark the order status
status_mapping = {
    'created': 1,
    'unavailable': 2,
    'canceled': 3,
    'approved': 4,
    'invoiced': 5,
    'processing': 6,
    'shipped': 7,
    'delivered': 8
}

#Convert text to numbers
orders['order_status_numeric'] = orders['order_status'].map(status_mapping)



payment_mapping = {
    'not_defined': 0,
    'voucher': 1,
    'boleto': 2,
    'credit_card': 3,
    'debit_card': 4
}
#Convert text to numbers
order_payments['payment_type_numeric'] = order_payments['payment_type'].map(payment_mapping)

"""##Calculate customers' loyalty
- As a group, we thought that this will be a nice addtion to differentiate customers based on loyalty using the link between unique id & customer id in order to merge this into the dataset


"""

# Merge customers with the loyalty information
customers['customer_loyalty'] = customers.groupby('customer_unique_id')['customer_id'].transform('count')

"""##Separate orders with one/mulitple review scores
- The code below seperates the orders with one review from those with multiple reviews using order id and checking duplictes and repeated orders as well.
- Right after that the results were converted to two dataframes and the average was calculated by using order id and review score.
- The final step was to merge all datasets as discussed in order to join both merged datasets into one final dataset that will be used to find all results
"""

# Create a dictionary with order_id as the key and corresponding rows as the value
order_reviews_dict = {}

# Traverse the data box and store each row by order_id
for index, row in order_reviews.iterrows():
    order_id = row['order_id']
    if order_id not in order_reviews_dict:
        order_reviews_dict[order_id] = []
    order_reviews_dict[order_id].append(row)

# Create two lists to store classification results
repeated_orders = []
unique_orders = []

# Traverse the dictionary to determine if order_id is duplicated
for order_id, rows in order_reviews_dict.items():
    if len(rows) > 1:
        repeated_orders.extend(rows)
    else:
        unique_orders.extend(rows)

repeated_orders_df = pd.DataFrame(repeated_orders)
unique_orders_df = pd.DataFrame(unique_orders)

average_repeated_orders = repeated_orders_df.groupby('order_id')['review_score'].mean().reset_index()

# Add English translations to the 'products' DataFrame
products = products.merge(
    translation[['product_category_name', 'product_category_name_english']],
    on='product_category_name',
    how='left'
)

# Generate the final datasets with translations included
repeated_orders_df.drop('review_score', axis=1, inplace=True)
repeated_orders_df = repeated_orders_df.merge(average_repeated_orders, on='order_id')

repeat_df = (
    repeated_orders_df
    .merge(orders, on='order_id', how='inner')  # Use modified 'orders' with calculated durations
    .merge(order_payments, on='order_id', how='inner')
    .merge(customers, on='customer_id', how='inner')
    .merge(order_items, on='order_id', how='inner')
    .merge(products, on='product_id', how='inner')  # 'products' now includes English translations
)

unique_df = (
    unique_orders_df
    .merge(orders, on='order_id', how='inner')  # Use modified 'orders' with calculated durations
    .merge(order_payments, on='order_id', how='inner')
    .merge(customers, on='customer_id', how='inner')
    .merge(order_items, on='order_id', how='inner')
    .merge(products, on='product_id', how='inner')  # 'products' now includes English translations
)

# Check the output
print(repeat_df[['product_category_name', 'product_category_name_english']].head())
print(unique_df[['product_category_name', 'product_category_name_english']].head())

"""##Joining datasets
- Below we can see that we dropped/removed the data we believe that will not be needed and finally merged both sets (unique df , repeat df) into one final set which is called final df



"""

selected_columns = [
    'review_score',
    'title_flag',
    'message_flag',
    'review_toanswer',
    'order_status_numeric',
    'customer_loyalty',

    'distance_km',
    'approval_duration_hours',
    'delivery_fromseller',
    'delivery_tocusto',
    'estimated_delivery_later',

    'payment_sequential',
    'payment_type',
    'payment_installments',
    'payment_value',
    'payment_type_numeric',

    #'customer_city',
    'customer_state',

    'order_item_id',
    'price',
    'freight_value',
    'product_name_lenght',
    'product_description_lenght',
    'product_photos_qty',
    'product_volumetric_weight_g',
    'volumetric_weight_rating',
    'weight_rating',
    'volume_rating',
    'descleng_rating'
]

#Select the desired column from repetiti_final.df
final_df1 = repeat_df[selected_columns]
final_df2 = unique_df[selected_columns]

#Total final dataset
final_df = pd.concat([final_df1,final_df2], ignore_index=True)

#Divide the final test dataset based on order status
nodelivery_df = final_df[final_df['order_status_numeric'].isin([1, 2, 3, 4, 5, 6])]
delivery_df = final_df[final_df['order_status_numeric'].isin([7,8])]

"""#Lasso

*   The lasso model, as the final step in feature engineering, aims to initially identify variables that significantly impact review scores.

☝However, the results show an MSE of 1.19, which is relatively high given that the target variable ranges from 1 to 5, making it less convincing and only useful as a reference. Therefore, we will continue to explore other models.

##Orders without delivery time
"""

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Lasso # Import Lasso class from sklearn.linear_model
from sklearn.model_selection import train_test_split # Import train_test_split
from sklearn.metrics import mean_squared_error # Import mean_squared_error


X1 = nodelivery_df.drop(['review_score'], axis=1)  # All features
y1 = nodelivery_df['review_score']  # Target variable

# Handling missing values
X1.fillna(0, inplace=True)
X1 = pd.get_dummies(X1, drop_first=True)  # Exclusive hot encoding

# Standardized features
scaler1 = StandardScaler()
X1_scaled = scaler1.fit_transform(X1)

X1_train, X1_test, y1_train, y1_test = train_test_split(X1_scaled, y1, test_size=0.2, random_state=42)

from sklearn.linear_model import LassoCV

lasso_cv = LassoCV(cv=5)  #cross validation to find best alpha
lasso_cv.fit(X1_train, y1_train)

lasso1 = Lasso(alpha= lasso_cv.alpha_) #using the best alpha value
lasso1.fit(X1_train, y1_train)

y1_pred = lasso1.predict(X1_test)
mse1 = mean_squared_error(y1_test, y1_pred)
print(f'Mean Squared Error: {mse1}')

coefficients = pd.DataFrame(lasso1.coef_, index=X1.columns, columns=['Coefficient'])
print(coefficients[coefficients['Coefficient'] != 0])

"""##Orders with delivery time"""

X2 = delivery_df.drop(['review_score'], axis=1)  # All features
y2 = delivery_df['review_score']  # Target variable

# Handling missing values
X2.fillna(0, inplace=True)
X2 = pd.get_dummies(X2, drop_first=True)  # Exclusive hot encoding

# Standardized features
scaler2 = StandardScaler()
X2_scaled = scaler2.fit_transform(X2)

X2_train, X2_test, y2_train, y2_test = train_test_split(X2_scaled, y2, test_size=0.2, random_state=42)

from sklearn.linear_model import LassoCV

lasso_cv = LassoCV(cv=5)  #cross validation
lasso_cv.fit(X2_train, y2_train)
print(f'Best alpha: {lasso_cv.alpha_}')

lasso2 = Lasso(alpha= lasso_cv.alpha_) #using the best alpha value
lasso2.fit(X2_train, y2_train)


y2_pred = lasso2.predict(X2_test)
mse2 = mean_squared_error(y2_test, y2_pred)
print(f'Mean Squared Error: {mse2}')


coefficients = pd.DataFrame(lasso2.coef_, index=X2.columns, columns=['Coefficient'])
print(coefficients[coefficients['Coefficient'] != 0])

"""# Models for orders without delivery time"""

from sklearn.ensemble import RandomForestRegressor as RF
from sklearn.ensemble import GradientBoostingRegressor as GBDT
from sklearn.metrics import mean_squared_error, r2_score
from xgboost import XGBRegressor as XGB
from sklearn.metrics import precision_recall_fscore_support

RF_algo = RF()
RF_model = RF_algo.fit(X1_train, y1_train)

GBDT_algo = GBDT()
GBDT_model = GBDT_algo.fit(X1_train, y1_train)

XGB_algo = XGB()
XGB_model = XGB_algo.fit(X1_train, y1_train)

models1 = [RF_model, GBDT_model, XGB_model]
names = ['Random Forest', 'Gradient Boosted Decision Trees', 'XGBDT']

for i in range(len(models1)):
  print(f"Model: {names[i]}")

  # predict based on training data
  predict = models1[i].predict(X1_train)

   # Calculate regression metrics (MSE and R-squared)
  mse = mean_squared_error(y1_train, predict)
  r2 = r2_score(y1_train, predict)
  print(f"Mean Squared Error: {mse}")
  print(f"R-squared: {r2}")
  print("\n")

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

# we get a load of warnings running the code so will supress them
import warnings
warnings.filterwarnings("ignore")

# create a hyperparameter search function for re-usability
def random_search(algo, hyperparameters, X_train, Y_train):
  # do the search using 5 folds/chunks
  clf = RandomizedSearchCV(algo, hyperparameters, cv=5, random_state=2015,
                          scoring='precision_macro', n_iter=20, refit=True)

  # pass the data to fit/train
  clf.fit(X_train, Y_train)

  return clf.best_params_

# Random Forest
RF_tuned_parameters = {
    'n_estimators': randint(50, 500), # Draw from a uniform distribution between 50 and 500
    'max_depth': randint(2, 7),  # Draw from a uniform distribution between 2 and 7
    'min_samples_split': randint(2, 7),  # Draw from a uniform distribution between 2 and 7
    'max_features': ['sqrt', 'log2', None]
}

RF_best_params = random_search(RF_algo, RF_tuned_parameters, X1_train, y1_train)

# GBDT
GBDT_tuned_parameters = {
    'n_estimators': randint(25, 250), # Draw from a uniform distribution between 50 and 500
    'learning_rate': uniform(loc=0.01, scale=4.99),  # Draw from a uniform distribution between 0.01 and 5
    'criterion': ['friedman_mse', 'squared_error'],
    'max_depth': randint(2, 7)  # Draw from a uniform distribution between 2 and 7
}

GBDT_best_params = random_search(GBDT_algo, GBDT_tuned_parameters, X1_train, y1_train)

# XGBDT
XGB_tuned_parameters = {
    'n_estimators': randint(25, 250), # Draw from a uniform distribution between 50 and 500
    # eta is learning rate
    'eta': uniform(loc=0.01, scale=4.99),  # Draw from a uniform distribution between 0.01 and 5
    # objective is the same as criterion
    'objective': ['binary:logistic', 'binary:hinge'],
    'max_depth': randint(2, 7)  # Draw from a uniform distribution between 2 and 7
}

XGB_best_params = random_search(XGB_algo, XGB_tuned_parameters, X1_train, y1_train)

"""Re-run after hyperparameter tuning"""

# score the models
models2 = [RF_model, GBDT_model, XGB_model] # redo this or it uses the old models

for i in range(len(models2)):
  print(f"Model: {names[i]}")

  # predict based on training data
  predict = models2[i].predict(X1_train)

  mse = mean_squared_error(y1_train, predict)
  r2 = r2_score(y1_train, predict)

  print(f"Mean Squared Error: {mse}")
  print(f"R-squared: {r2}")
  print("\n")

for i in range(len(models2)):
  print(f"Model: {names[i]}")

  # predict based on TEST data
  predict = models2[i].predict(X1_test)

  # Calculate regression metrics (MSE and R-squared)
  mse = mean_squared_error(y1_test, predict)
  r2 = r2_score(y1_test, predict)
  print(f"Mean Squared Error: {mse}")
  print(f"R-squared: {r2}")
  print("\n")

"""# Models for Orders with delivery time"""

from sklearn.ensemble import RandomForestRegressor as RF
from sklearn.ensemble import GradientBoostingRegressor as GBDT
from sklearn.metrics import mean_squared_error, r2_score
from xgboost import XGBRegressor as XGB
from sklearn.metrics import precision_recall_fscore_support

RF_algo = RF()
RF_model_2 = RF_algo.fit(X2_train, y2_train)

GBDT_algo = GBDT()
GBDT_model_2 = GBDT_algo.fit(X2_train, y2_train)

XGB_algo = XGB()
XGB_model_2 = XGB_algo.fit(X2_train, y2_train)

models3 = [RF_model_2, GBDT_model_2, XGB_model_2]
names = ['Random Forest', 'Gradient Boosted Decision Trees', 'XGBDT']

for i in range(len(models3)):
  print(f"Model: {names[i]}")

  # predict based on training data
  predict = models3[i].predict(X2_train)

   # Calculate regression metrics (MSE and R-squared)
  mse = mean_squared_error(y2_train, predict)
  r2 = r2_score(y2_train, predict)
  print(f"Mean Squared Error: {mse}")
  print(f"R-squared: {r2}")
  print("\n")

for i in range(len(models3)):
  print(f"Model: {names[i]}")

  # predict based on TEST data
  predict = models3[i].predict(X2_test)

  # Calculate regression metrics (MSE and R-squared)
  mse = mean_squared_error(y2_test, predict)
  r2 = r2_score(y2_test, predict)
  print(f"Mean Squared Error: {mse}")
  print(f"R-squared: {r2}")
  print("\n")

#To predict on new, unseen data:
new_data = pd.DataFrame({'feature1': [X2_test], 'feature2': [X1_test],})
new_predictions_rf_2 = RF_model_2.predict(X2_test)
new_predictions_rf = RF_model.predict(X1_test)
new_predictions_rf_2 = new_predictions_rf_2.round()
new_predictions_rf = new_predictions_rf.round()
print(new_predictions_rf_2)
print(new_predictions_rf)

print(new_predictions_rf.round())

"""##Feature Importance

*   Regarding the variables of review, such as ***title_flag***, ***message_flag***, and the duration of responding to invitation reviews '***review_toanswer***',

☝If their importance ranks high in the model, it can be concluded that **using the habit of writing reviews to draw user profiles can identify customers who habitually score high, and later send invitation reviews specifically**
"""

import pandas as pd
import matplotlib.pyplot as plt

# Obtain feature importance
importances = RF_model.feature_importances_
feature_names = X1.columns

importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})

# Sort by importance
importance_df = importance_df.sort_values(by='Importance', ascending=False)
print(importance_df)

importances2 = RF_model_2.feature_importances_

feature_names2 = X2.columns
importance_df2 = pd.DataFrame({'Feature': feature_names2, 'Importance': importances2})

importance_df2 = importance_df2.sort_values(by='Importance', ascending=False)

print(importance_df2)

"""

---